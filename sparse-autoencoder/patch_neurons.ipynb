{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23da2340-45c6-4e83-b8ae-1020b6b6b344",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbcd0dd3-b33e-496e-a5bb-834b0d408f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer Configuration ---\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "REPLACEMENT_LAYER_IDX = 3\n",
    "LAYER_NAME = f\"model.layers.{REPLACEMENT_LAYER_IDX}.mlp.down_proj\"\n",
    "ACTIVATION_DIM = 1152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5c4d8b-e402-404e-891c-83811ee4cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAE Configuration ---\n",
    "SAE_EXPANSION_FACTOR = 8 # How many times larger the SAE hidden dim is than the activation dim\n",
    "SAE_HIDDEN_DIM = ACTIVATION_DIM * SAE_EXPANSION_FACTOR\n",
    "L1_COEFF = 3e-4 # Sparsity penalty strength\n",
    "CHECKPOINT_PATH = \"runs/wikitext/B_google_gemma-3-1b-it_model.layers.3.mlp.down_proj_sae_training_logs_20250514-113504\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e3448a-01df-456f-9130-91a2a6963b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- General Configuration ---\n",
    "from secret_tokens import access_tokens\n",
    "token = access_tokens[\"hf\"]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma3ForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm  # Use auto version for notebook compatibility\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84af7c4-4690-4e3d-9d7a-23cd699fe7a4",
   "metadata": {},
   "source": [
    "# Prepare LLM and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494bd13e-7009-45b3-95ed-bc3443152378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A1000 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b3656b9-e9b5-468a-86e7-84ab93997a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SAE with ACTIVATION_DIM=1152, SAE_HIDDEN_DIM=9216\n",
      "Checkpoint loaded successfully from runs/wikitext/B_google_gemma-3-1b-it_model.layers.3.mlp.down_proj_sae_training_logs_20250514-113504/sae_google_gemma-3-1b-it_model.layers.3.mlp.down_proj.pth\n",
      "SAE Model:\n",
      "SparseAutoencoder(\n",
      "  (encoder): Linear(in_features=1152, out_features=9216, bias=True)\n",
      "  (decoder): Linear(in_features=9216, out_features=1152, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim, bias=True)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.relu(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "    def encode(self, x):\n",
    "      return self.relu(self.encoder(x))\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Initialize SAE\n",
    "print(f\"Initializing SAE with ACTIVATION_DIM={ACTIVATION_DIM}, SAE_HIDDEN_DIM={SAE_HIDDEN_DIM}\")\n",
    "sae_model = SparseAutoencoder(ACTIVATION_DIM, SAE_HIDDEN_DIM).to(device)\n",
    "\n",
    "checkpoint_full_path = f\"{CHECKPOINT_PATH}/sae_{MODEL_NAME.replace('/','_')}_{LAYER_NAME}.pth\"\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_full_path, map_location=device)\n",
    "    sae_model.load_state_dict(checkpoint[\"sae_model_state_dict\"])\n",
    "    print(f\"Checkpoint loaded successfully from {checkpoint_full_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Checkpoint file not found at {checkpoint_full_path}\")\n",
    "    print(\"Please ensure the path is correct.\")\n",
    "    # Exit or raise error if file not found, as further steps will fail\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load checkpoint from {checkpoint_full_path}. Error: {e}\")\n",
    "    raise\n",
    "print(\"SAE Model:\")\n",
    "print(sae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799940dd-76bc-47dc-8841-98634bd12d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_mean loaded. Shape: torch.Size([1, 1152]), Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load act mean\n",
    "if 'act_mean' in checkpoint:\n",
    "    loaded_act_mean = checkpoint['act_mean']\n",
    "    loaded_act_mean = loaded_act_mean.to(device) # Ensure it's on the same device\n",
    "    print(f\"act_mean loaded. Shape: {loaded_act_mean.shape}, Device: {loaded_act_mean.device}\")\n",
    "else:\n",
    "    raise KeyError(\"'act_mean' or 'act norms' missing from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8630ede4-e9d3-4d6f-b5af-c7030fb54969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper class to allow the patching\n",
    "class SAEIntervenableMLP(nn.Module):\n",
    "    def __init__(self, sae_model_instance: SparseAutoencoder, act_mean_global_tensor: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.sae = sae_model_instance\n",
    "        self.sae.eval()\n",
    "\n",
    "        if act_mean_global_tensor.ndim == 1:\n",
    "            act_mean_global_tensor = act_mean_global_tensor.unsqueeze(0)\n",
    "        if act_mean_global_tensor.shape[0] != 1:\n",
    "            raise ValueError(f\"act_mean_global_tensor should have shape [1, dim] or [dim], got {act_mean_global_tensor.shape}\")\n",
    "\n",
    "        self.register_buffer('act_mean_global', act_mean_global_tensor)\n",
    "        self.normalization_epsilon = 1e-6 # Same as used in your SAE training data prep\n",
    "\n",
    "        self.patch_fn = None\n",
    "        self.patch_kwargs = None\n",
    "\n",
    "    def set_patch_fn(self, patch_fn=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Set a function to modify the SAE's encoded features.\n",
    "        The patch_fn should take `encoded_features` as the first argument,\n",
    "        and any additional `kwargs`.\n",
    "        Example: def my_patch(features, idx_to_ablate): features[:, idx_to_ablate] = 0; return features\n",
    "        set_patch_fn(my_patch, idx_to_ablate=10)\n",
    "        \"\"\"\n",
    "        self.patch_fn = patch_fn\n",
    "        self.patch_kwargs = kwargs if kwargs else {}\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: Input activations (e.g., from the layer norm before the original MLP)\n",
    "           Shape: (batch_size, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(f\"Input tensor x must be 3-dimensional (batch, seq, dim), got {x.ndim}\")\n",
    "        \n",
    "        original_shape = x.shape\n",
    "        hidden_dim = x.shape[-1]\n",
    "        \n",
    "        x_flat = x.reshape(-1, hidden_dim)\n",
    "\n",
    "        # Normalize input in the same way activations were normalized for SAE training\n",
    "        # (x - mean) / norm\n",
    "        # self.act_mean_global is (1, hidden_dim)\n",
    "        x_centered = x_flat - self.act_mean_global\n",
    "        act_norms = torch.norm(x_centered, dim=1, keepdim=True) + self.normalization_epsilon\n",
    "        x_normalized = x_centered / act_norms\n",
    "\n",
    "        encoded_features = self.sae.encode(x_normalized)\n",
    "\n",
    "        # Apply patching\n",
    "        if self.patch_fn is not None:\n",
    "            patched_features = self.patch_fn(encoded_features, **self.patch_kwargs)\n",
    "        else:\n",
    "            patched_features = encoded_features\n",
    "        \n",
    "        decoded_normalized = self.sae.decode(patched_features)\n",
    "\n",
    "        # Denormalize: output = reconstructed_normalized * original_norm + original_mean\n",
    "        # This aims to restore the scale and shift of the input `x` to the SAEIntervenableMLP.\n",
    "        # Note: The SAE was trained to reconstruct normalized *MLP outputs*. Here we are\n",
    "        # (de)normalizing based on the statistics of *MLP inputs*. This is an approximation.\n",
    "        reconstructed_output_flat = decoded_normalized * act_norms + self.act_mean_global\n",
    "        \n",
    "        # Reshape to original input shape\n",
    "        output = reconstructed_output_flat.reshape(original_shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6622b449-fdd4-403f-b33e-6d803667302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-3-1b-it\n",
      "Loading tokenizer: google/gemma-3-1b-it\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device) # for gpt2\n",
    "model = Gemma3ForCausalLM.from_pretrained(MODEL_NAME, token=token) # for gemma\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae417929-3829-4b09-b2d0-eb4ca599a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mlp_layer_with_sae_mlp(gemma_model: Gemma3ForCausalLM,\n",
    "                                   layer_idx: int,\n",
    "                                   sae_inter_mlp_instance: SAEIntervenableMLP) -> Gemma3ForCausalLM: # Or GemmaForCausalLM\n",
    "    \"\"\"\n",
    "    Replaces the MLP block in a specific layer of the Gemma model with the SAEIntervenableMLP.\n",
    "    Returns a deep copy of the model with the layer replaced.\n",
    "    \"\"\"\n",
    "    model_copy = copy.deepcopy(gemma_model)\n",
    "    model_copy.eval()\n",
    "\n",
    "    if not (0 <= layer_idx < len(model_copy.model.layers)):\n",
    "        raise ValueError(f\"Layer index {layer_idx} is out of bounds for model with {len(model_copy.model.layers)} layers.\")\n",
    "\n",
    "    # Insert the SAE after the down proj\n",
    "    original_mlp_down_proj = model_copy.model.layers[layer_idx].mlp.down_proj\n",
    "    \n",
    "    # Ensure the SAEIntervenableMLP is on the same device as the model\n",
    "    sae_inter_mlp_instance = sae_inter_mlp_instance.to(next(model_copy.parameters()).device)\n",
    "    \n",
    "    model_copy.model.layers[layer_idx].mlp.down_proj = nn.Sequential(\n",
    "        original_mlp_down_proj,\n",
    "        sae_inter_mlp_instance,\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Replaced MLP in layer {layer_idx} of Gemma model with SAEIntervenableMLP.\")\n",
    "    print(f\"  Original MLP: {gemma_model.model.layers[layer_idx].mlp}\")\n",
    "    print(f\"  New MLP: {model_copy.model.layers[layer_idx].mlp}\")\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d98ff9-a0f6-4d70-9df1-ce9d9f5d8117",
   "metadata": {},
   "source": [
    "# Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec627bbd-f0ad-4cd0-aa05-a4b1fe421c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_inter_mlp = SAEIntervenableMLP(sae_model, loaded_act_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5528bc3-6414-42a5-bba3-bb597b6dbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced MLP in layer 3 of Gemma model with SAEIntervenableMLP.\n",
      "  Original MLP: Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "  (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "  (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n",
      "  New MLP: Gemma3MLP(\n",
      "  (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "  (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "  (down_proj): Sequential(\n",
      "    (0): Linear(in_features=6912, out_features=1152, bias=False)\n",
      "    (1): SAEIntervenableMLP(\n",
      "      (sae): SparseAutoencoder(\n",
      "        (encoder): Linear(in_features=1152, out_features=9216, bias=True)\n",
      "        (decoder): Linear(in_features=9216, out_features=1152, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (act_fn): PytorchGELUTanh()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "      (3): Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Sequential(\n",
       "            (0): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "            (1): SAEIntervenableMLP(\n",
       "              (sae): SparseAutoencoder(\n",
       "                (encoder): Linear(in_features=1152, out_features=9216, bias=True)\n",
       "                (decoder): Linear(in_features=9216, out_features=1152, bias=True)\n",
       "                (relu): ReLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "      (4-25): 22 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_sae_mlp = replace_mlp_layer_with_sae_mlp(model, REPLACEMENT_LAYER_IDX, sae_inter_mlp)\n",
    "model_with_sae_mlp.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e5ac13-f7a0-4a89-905b-48ad8be4e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate text\n",
    "def generate_text(model_to_use, tokenizer_to_use, prompt, max_new_tokens=50):\n",
    "    model_to_use.eval() # Ensure model is in eval mode\n",
    "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model_to_use.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_use.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            #eos_token_id=tokenizer_to_use.eos_token_id, # Optional: helps stop generation\n",
    "            #pad_token_id=tokenizer_to_use.pad_token_id, # Ensure this is set\n",
    "            do_sample=False # For more comparable deterministic output\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer_to_use.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6360dcbb-b2c5-4ccf-8f20-ef28589aab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with Original Gemma Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd23297/Documents/mechanistic-interpretability/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/sd23297/Documents/mechanistic-interpretability/env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I walked for 3 km straight. Last year, I run a marathon that was long \n",
      "Original Gemma Output: I walked for 3 km straight. Last year, I run a marathon that was long 42.195 km.\n",
      "\n",
      "The question asks for the distance of the walk.\n",
      "\n",
      "The answer is 3 km.\n",
      "\n",
      "Final Answer: The final answer is $\\boxed{3}$\n",
      "\n",
      "--- Generating with Gemma + SAE-MLP (Layer 3, No Patching) ---\n",
      "Prompt: I walked for 3 km straight. Last year, I run a marathon that was long \n",
      "Gemma + SAE-MLP (No Patch) Output: I walked for 3 km straight. Last year, I run a marathon that was long 42.195 km.\n",
      "\n",
      "The question asks for the distance of the walk.\n",
      "\n",
      "The answer is 3 km.\n",
      "\n",
      "Final Answer: The final answer is $\\boxed{3}$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I walked for 3 km straight. Last year, I run a marathon that was long \"\n",
    "\n",
    "print(\"--- Generating with Original Gemma Model ---\")\n",
    "original_output = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Original Gemma Output: {original_output}\\n\")\n",
    "\n",
    "print(f\"--- Generating with Gemma + SAE-MLP (Layer {REPLACEMENT_LAYER_IDX}, No Patching) ---\")\n",
    "sae_mlp_no_patch_output = generate_text(model_with_sae_mlp, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Gemma + SAE-MLP (No Patch) Output: {sae_mlp_no_patch_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d47db7d-2d5b-42f4-b8d9-320bb9ac3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_features_patch(encoded_features, feature_idx):\n",
    "    \"\"\"Sets specified feature activations to zero.\"\"\"\n",
    "    patched = encoded_features.clone()\n",
    "    if not isinstance(feature_idx, list):\n",
    "        feature_idx = [feature_idx]\n",
    "    for idx in feature_idx:\n",
    "        patched[:, idx] = 0.0\n",
    "    return patched\n",
    "\n",
    "def amplify_feature_patch(encoded_features, feature_idx, scale_factor):\n",
    "    \"\"\"Amplifies a specified feature activation.\"\"\"\n",
    "    patched = encoded_features.clone()\n",
    "    patched[:, feature_idx] *= scale_factor\n",
    "    return patched\n",
    "\n",
    "def set_feature_value_patch(encoded_features, feature_idx, value):\n",
    "    \"\"\"Sets a specified feature activation to a fixed value.\"\"\"\n",
    "    patched = encoded_features.clone()\n",
    "    patched[:, feature_idx] = value\n",
    "    return patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86920470-a5cb-4859-bad5-64b8c5a6356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with Gemma + SAE-MLP (Layer 3, Ablating Feature 2715) ---\n",
      "Prompt: I walked for 3 km straight. Last year, I run a marathon that was long \n",
      "Gemma + SAE-MLP (Ablating F2715) Output: I walked for 3 km straight. Last year, I run a marathon that was long 10 years.\n",
      "I'm a small business, and I'm struggling to make a profit.\n",
      "I'm feeling a bit lost.\n",
      "\n",
      "I've been working on a new marketing strategy.\n",
      "I'm trying to build\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sae_mlp_layer_in_model = model_with_sae_mlp.model.layers[REPLACEMENT_LAYER_IDX].mlp.down_proj[1]\n",
    "\n",
    "feature_to_patch = 2715\n",
    "sae_mlp_layer_in_model.set_patch_fn(set_feature_value_patch, feature_idx=feature_to_patch, value=0.5)\n",
    "\n",
    "print(f\"--- Generating with Gemma + SAE-MLP (Layer {REPLACEMENT_LAYER_IDX}, Ablating Feature {feature_to_patch}) ---\")\n",
    "sae_mlp_ablation_output = generate_text(model_with_sae_mlp, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Gemma + SAE-MLP (Ablating F{feature_to_patch}) Output: {sae_mlp_ablation_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaeb9838-22d6-40df-a718-71a8a53749ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean model after patching\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de53cd-04a1-47f4-be84-eeadcfe92513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
